split crawler & indexer into two processes

CRAWLER:
✅ allowed_hostnames : list
✅ queue: list

✅ fetch allowed hostnames from db
✅ fetch queue from db

✅ url = first item in queue
✅ contents = fetch contents of url
hyperlinks = find all hyperlinks in contents

known pages = fetch all urls from known index that are also in the hyperlinks

for each hyperlink:
    if links to page outside of allowed hostnames:
        skip
    if already in queue:
        skip
    if links to a known page:
        skip
    if links to a variation of a known page:
        skip

    add to queue

add url to known pages
remove from queue